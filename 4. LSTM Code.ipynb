{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BiLSTM Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T15:52:09.618663Z",
     "iopub.status.busy": "2025-07-20T15:52:09.618361Z",
     "iopub.status.idle": "2025-07-20T15:55:31.637694Z",
     "shell.execute_reply": "2025-07-20T15:55:31.636929Z",
     "shell.execute_reply.started": "2025-07-20T15:52:09.618628Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext==0.10.0\n",
      "  Downloading torchtext-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.6 MB 14.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext==0.10.0) (1.18.5)\n",
      "Collecting torch==1.9.0\n",
      "  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 831.4 MB 2.0 kB/s ta 0:00:0101��█▊              | 459.5 MB 77 kB/s eta 1:19:57��████████████████████████▋   | 742.9 MB 65 kB/s eta 0:22:3108.9 MB 1.3 MB/s eta 0:00:18\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext==0.10.0) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext==0.10.0) (4.45.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.9.0->torchtext==0.10.0) (3.7.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.10.0) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.10.0) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.10.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.10.0) (2.9)\n",
      "\u001b[31mERROR: kornia 0.3.1 has requirement torch==1.5.0, but you'll have torch 1.9.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: allennlp 1.0.0 has requirement torch<1.6.0,>=1.5.0, but you'll have torch 1.9.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: torch, torchtext\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.5.1\n",
      "    Uninstalling torch-1.5.1:\n",
      "      Successfully uninstalled torch-1.5.1\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.6.0\n",
      "    Uninstalling torchtext-0.6.0:\n",
      "      Successfully uninstalled torchtext-0.6.0\n",
      "Successfully installed torch-1.9.0 torchtext-0.10.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.7/site-packages (2.2.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy) (46.1.3.post20200325)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.18.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (4.45.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (7.4.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /opt/conda/lib/python3.7/site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /opt/conda/lib/python3.7/site-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.1.3.post20200325)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.45.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.7.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: thinc==7.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext==0.10.0\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T15:57:46.791851Z",
     "iopub.status.busy": "2025-07-20T15:57:46.791556Z",
     "iopub.status.idle": "2025-07-20T15:57:47.365740Z",
     "shell.execute_reply": "2025-07-20T15:57:47.364908Z",
     "shell.execute_reply.started": "2025-07-20T15:57:46.791826Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.legacy import data\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "SEED = 32\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"/kaggle/input/hate-speech-and-offensive-language-dataset/labeled_data.csv\")\n",
    "df = df[[\"tweet\", \"class\"]]\n",
    "df[\"label\"] = df[\"class\"].apply(lambda x: 0 if x == 2 else 1)\n",
    "df = df[[\"tweet\", \"label\"]]  # Keep only needed columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T15:57:57.165432Z",
     "iopub.status.busy": "2025-07-20T15:57:57.165152Z",
     "iopub.status.idle": "2025-07-20T15:57:57.171291Z",
     "shell.execute_reply": "2025-07-20T15:57:57.170357Z",
     "shell.execute_reply.started": "2025-07-20T15:57:57.165408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def tokenize_and_clean(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", '', text)\n",
    "    text = text.lower()\n",
    "    return [token.lemma_ for token in spacy_en.tokenizer(text)\n",
    "            if token.text not in STOP_WORDS and token.text.strip() != \"\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T15:58:04.649906Z",
     "iopub.status.busy": "2025-07-20T15:58:04.649575Z",
     "iopub.status.idle": "2025-07-20T15:58:12.130091Z",
     "shell.execute_reply": "2025-07-20T15:58:12.129451Z",
     "shell.execute_reply.started": "2025-07-20T15:58:04.649865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize=tokenize_and_clean, lower=True, batch_first=True, fix_length=100)\n",
    "LABEL = data.LabelField(dtype=torch.float, batch_first=True)\n",
    "\n",
    "class DataFrameDataset(data.Dataset):\n",
    "    def __init__(self, df, text_field, label_field, **kwargs):\n",
    "        fields = [('text', text_field), ('label', label_field)]\n",
    "        examples = [data.Example.fromlist([row['tweet'], row['label']], fields) for _, row in df.iterrows()]\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=SEED)\n",
    "train_dataset = DataFrameDataset(train_df, TEXT, LABEL)\n",
    "train_data, val_data = train_dataset.split(split_ratio=0.8, random_state=random.seed(SEED))\n",
    "test_dataset = DataFrameDataset(test_df, TEXT, LABEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T15:58:20.069974Z",
     "iopub.status.busy": "2025-07-20T15:58:20.069662Z",
     "iopub.status.idle": "2025-07-20T16:01:51.161549Z",
     "shell.execute_reply": "2025-07-20T16:01:51.160618Z",
     "shell.execute_reply.started": "2025-07-20T15:58:20.069947Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [02:58, 4.83MB/s]                               \n",
      "100%|█████████▉| 399999/400000 [00:14<00:00, 27774.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 4697\n",
      "Common tokens: [('bitch', 7167), ('rt', 4881), ('not', 2961), ('hoe', 2741), ('get', 2208), ('like', 1811), ('pussy', 1484), ('fuck', 1471), ('go', 1003), ('ass', 993)]\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data, min_freq=3, vectors=\"glove.6B.100d\")\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(\"Vocab size:\", len(TEXT.vocab))\n",
    "print(\"Common tokens:\", TEXT.vocab.freqs.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T16:02:35.536081Z",
     "iopub.status.busy": "2025-07-20T16:02:35.535782Z",
     "iopub.status.idle": "2025-07-20T16:02:35.540354Z",
     "shell.execute_reply": "2025-07-20T16:02:35.539582Z",
     "shell.execute_reply.started": "2025-07-20T16:02:35.536056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, val_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, val_data, test_dataset),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device,\n",
    "    sort=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T16:02:44.728651Z",
     "iopub.status.busy": "2025-07-20T16:02:44.728336Z",
     "iopub.status.idle": "2025-07-20T16:02:47.624241Z",
     "shell.execute_reply": "2025-07-20T16:02:47.623377Z",
     "shell.execute_reply.started": "2025-07-20T16:02:44.728622Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (embedding): Embedding(4697, 100, padding_idx=1)\n",
       "  (lstm): LSTM(100, 128, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        out = self.dropout(torch.cat((lstm_out[:, -1, :self.lstm.hidden_size],\n",
    "                                      lstm_out[:, 0, self.lstm.hidden_size:]), dim=1))\n",
    "        return torch.sigmoid(self.fc(out))\n",
    "\n",
    "model = BiLSTM(len(TEXT.vocab), 100, 128, 1, TEXT.vocab.stoi[TEXT.pad_token])\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T16:03:46.694186Z",
     "iopub.status.busy": "2025-07-20T16:03:46.693902Z",
     "iopub.status.idle": "2025-07-20T16:03:46.704429Z",
     "shell.execute_reply": "2025-07-20T16:03:46.703577Z",
     "shell.execute_reply.started": "2025-07-20T16:03:46.694162Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.BCELoss()\n",
    "criterion.to(device)\n",
    "\n",
    "def train(model, iterator):\n",
    "    model.train()\n",
    "    epoch_loss, all_preds, all_labels = 0, [], []\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch.text).squeeze()\n",
    "        loss = criterion(pred, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        all_preds += torch.round(pred).cpu().tolist()\n",
    "        all_labels += batch.label.cpu().tolist()\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return epoch_loss / len(iterator), acc, f1\n",
    "\n",
    "def evaluate(model, iterator):\n",
    "    model.eval()\n",
    "    epoch_loss, all_preds, all_labels = 0, [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            pred = model(batch.text).squeeze()\n",
    "            loss = criterion(pred, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            all_preds += torch.round(pred).cpu().tolist()\n",
    "            all_labels += batch.label.cpu().tolist()\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return epoch_loss / len(iterator), acc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T16:03:54.806011Z",
     "iopub.status.busy": "2025-07-20T16:03:54.805710Z",
     "iopub.status.idle": "2025-07-20T16:06:07.322357Z",
     "shell.execute_reply": "2025-07-20T16:06:07.321492Z",
     "shell.execute_reply.started": "2025-07-20T16:03:54.805986Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Loss: 0.0764 | Acc: 0.9691 | F1: 0.9080\n",
      "Val   Loss: 0.1612 | Acc: 0.9415 | F1: 0.8307\n",
      "Epoch 2\n",
      "Train Loss: 0.0653 | Acc: 0.9753 | F1: 0.9258\n",
      "Val   Loss: 0.1810 | Acc: 0.9377 | F1: 0.8113\n",
      "Epoch 3\n",
      "Train Loss: 0.0599 | Acc: 0.9782 | F1: 0.9347\n",
      "Val   Loss: 0.1789 | Acc: 0.9400 | F1: 0.8208\n",
      "Epoch 4\n",
      "Train Loss: 0.0518 | Acc: 0.9818 | F1: 0.9452\n",
      "Val   Loss: 0.2104 | Acc: 0.9415 | F1: 0.8368\n",
      "Epoch 5\n",
      "Train Loss: 0.0466 | Acc: 0.9841 | F1: 0.9521\n",
      "Val   Loss: 0.2033 | Acc: 0.9377 | F1: 0.8139\n",
      "Epoch 6\n",
      "Train Loss: 0.0377 | Acc: 0.9875 | F1: 0.9623\n",
      "Val   Loss: 0.2265 | Acc: 0.9397 | F1: 0.8264\n",
      "Epoch 7\n",
      "Train Loss: 0.0362 | Acc: 0.9883 | F1: 0.9648\n",
      "Val   Loss: 0.2247 | Acc: 0.9339 | F1: 0.8045\n",
      "Epoch 8\n",
      "Train Loss: 0.0367 | Acc: 0.9873 | F1: 0.9616\n",
      "Val   Loss: 0.2116 | Acc: 0.9372 | F1: 0.8109\n",
      "Epoch 9\n",
      "Train Loss: 0.0277 | Acc: 0.9907 | F1: 0.9721\n",
      "Val   Loss: 0.2562 | Acc: 0.9359 | F1: 0.8157\n",
      "Epoch 10\n",
      "Train Loss: 0.0334 | Acc: 0.9888 | F1: 0.9662\n",
      "Val   Loss: 0.2393 | Acc: 0.9367 | F1: 0.8214\n",
      "Epoch 11\n",
      "Train Loss: 0.0277 | Acc: 0.9909 | F1: 0.9726\n",
      "Val   Loss: 0.2659 | Acc: 0.9397 | F1: 0.8191\n",
      "Epoch 12\n",
      "Train Loss: 0.0219 | Acc: 0.9923 | F1: 0.9769\n",
      "Val   Loss: 0.2846 | Acc: 0.9359 | F1: 0.8046\n",
      "Epoch 13\n",
      "Train Loss: 0.0186 | Acc: 0.9936 | F1: 0.9808\n",
      "Val   Loss: 0.2848 | Acc: 0.9349 | F1: 0.8066\n",
      "Epoch 14\n",
      "Train Loss: 0.0149 | Acc: 0.9952 | F1: 0.9855\n",
      "Val   Loss: 0.3225 | Acc: 0.9342 | F1: 0.8102\n",
      "Epoch 15\n",
      "Train Loss: 0.0185 | Acc: 0.9939 | F1: 0.9818\n",
      "Val   Loss: 0.3005 | Acc: 0.9380 | F1: 0.8153\n",
      "Epoch 16\n",
      "Train Loss: 0.0158 | Acc: 0.9953 | F1: 0.9859\n",
      "Val   Loss: 0.3284 | Acc: 0.9369 | F1: 0.8123\n",
      "Epoch 17\n",
      "Train Loss: 0.0151 | Acc: 0.9953 | F1: 0.9857\n",
      "Val   Loss: 0.3228 | Acc: 0.9329 | F1: 0.7909\n",
      "Epoch 18\n",
      "Train Loss: 0.0158 | Acc: 0.9948 | F1: 0.9842\n",
      "Val   Loss: 0.3469 | Acc: 0.9337 | F1: 0.7966\n",
      "Epoch 19\n",
      "Train Loss: 0.0158 | Acc: 0.9945 | F1: 0.9834\n",
      "Val   Loss: 0.2956 | Acc: 0.9359 | F1: 0.8046\n",
      "Epoch 20\n",
      "Train Loss: 0.0143 | Acc: 0.9954 | F1: 0.9861\n",
      "Val   Loss: 0.3286 | Acc: 0.9397 | F1: 0.8231\n",
      "Epoch 21\n",
      "Train Loss: 0.0171 | Acc: 0.9945 | F1: 0.9833\n",
      "Val   Loss: 0.3536 | Acc: 0.9359 | F1: 0.8116\n",
      "Epoch 22\n",
      "Train Loss: 0.0166 | Acc: 0.9949 | F1: 0.9846\n",
      "Val   Loss: 0.3332 | Acc: 0.9354 | F1: 0.8084\n",
      "Epoch 23\n",
      "Train Loss: 0.0096 | Acc: 0.9970 | F1: 0.9911\n",
      "Val   Loss: 0.3481 | Acc: 0.9354 | F1: 0.8049\n",
      "Epoch 24\n",
      "Train Loss: 0.0076 | Acc: 0.9975 | F1: 0.9924\n",
      "Val   Loss: 0.3913 | Acc: 0.9354 | F1: 0.8043\n",
      "Epoch 25\n",
      "Train Loss: 0.0081 | Acc: 0.9972 | F1: 0.9914\n",
      "Val   Loss: 0.4476 | Acc: 0.9357 | F1: 0.8101\n",
      "Epoch 26\n",
      "Train Loss: 0.0068 | Acc: 0.9975 | F1: 0.9924\n",
      "Val   Loss: 0.4800 | Acc: 0.9385 | F1: 0.8174\n",
      "Epoch 27\n",
      "Train Loss: 0.0071 | Acc: 0.9975 | F1: 0.9926\n",
      "Val   Loss: 0.4361 | Acc: 0.9319 | F1: 0.7929\n",
      "Epoch 28\n",
      "Train Loss: 0.0253 | Acc: 0.9917 | F1: 0.9751\n",
      "Val   Loss: 0.3492 | Acc: 0.9324 | F1: 0.8006\n",
      "Epoch 29\n",
      "Train Loss: 0.0116 | Acc: 0.9964 | F1: 0.9892\n",
      "Val   Loss: 0.4660 | Acc: 0.9339 | F1: 0.8000\n",
      "Epoch 30\n",
      "Train Loss: 0.0109 | Acc: 0.9963 | F1: 0.9888\n",
      "Val   Loss: 0.4063 | Acc: 0.9296 | F1: 0.7801\n",
      "Epoch 31\n",
      "Train Loss: 0.0072 | Acc: 0.9977 | F1: 0.9930\n",
      "Val   Loss: 0.4544 | Acc: 0.9352 | F1: 0.8101\n",
      "Epoch 32\n",
      "Train Loss: 0.0063 | Acc: 0.9977 | F1: 0.9932\n",
      "Val   Loss: 0.4655 | Acc: 0.9352 | F1: 0.8081\n",
      "Epoch 33\n",
      "Train Loss: 0.0058 | Acc: 0.9980 | F1: 0.9939\n",
      "Val   Loss: 0.5294 | Acc: 0.9380 | F1: 0.8210\n",
      "Epoch 34\n",
      "Train Loss: 0.0060 | Acc: 0.9978 | F1: 0.9934\n",
      "Val   Loss: 0.4921 | Acc: 0.9299 | F1: 0.7878\n",
      "Epoch 35\n",
      "Train Loss: 0.0058 | Acc: 0.9979 | F1: 0.9937\n",
      "Val   Loss: 0.5742 | Acc: 0.9311 | F1: 0.7930\n",
      "Epoch 36\n",
      "Train Loss: 0.0063 | Acc: 0.9976 | F1: 0.9928\n",
      "Val   Loss: 0.5777 | Acc: 0.9347 | F1: 0.8024\n",
      "Epoch 37\n",
      "Train Loss: 0.0054 | Acc: 0.9981 | F1: 0.9943\n",
      "Val   Loss: 0.5856 | Acc: 0.9337 | F1: 0.8021\n",
      "Epoch 38\n",
      "Train Loss: 0.0058 | Acc: 0.9977 | F1: 0.9930\n",
      "Val   Loss: 0.6415 | Acc: 0.9369 | F1: 0.8123\n",
      "Epoch 39\n",
      "Train Loss: 0.0201 | Acc: 0.9933 | F1: 0.9796\n",
      "Val   Loss: 0.3136 | Acc: 0.9405 | F1: 0.8272\n",
      "Epoch 40\n",
      "Train Loss: 0.0134 | Acc: 0.9961 | F1: 0.9882\n",
      "Val   Loss: 0.4131 | Acc: 0.9357 | F1: 0.8113\n",
      "Epoch 41\n",
      "Train Loss: 0.0085 | Acc: 0.9968 | F1: 0.9903\n",
      "Val   Loss: 0.4115 | Acc: 0.9380 | F1: 0.8175\n",
      "Epoch 42\n",
      "Train Loss: 0.0129 | Acc: 0.9953 | F1: 0.9858\n",
      "Val   Loss: 0.3601 | Acc: 0.9387 | F1: 0.8188\n",
      "Epoch 43\n",
      "Train Loss: 0.0082 | Acc: 0.9975 | F1: 0.9924\n",
      "Val   Loss: 0.3833 | Acc: 0.9372 | F1: 0.8132\n",
      "Epoch 44\n",
      "Train Loss: 0.0059 | Acc: 0.9977 | F1: 0.9930\n",
      "Val   Loss: 0.4578 | Acc: 0.9369 | F1: 0.8143\n",
      "Epoch 45\n",
      "Train Loss: 0.0084 | Acc: 0.9971 | F1: 0.9913\n",
      "Val   Loss: 0.5171 | Acc: 0.9294 | F1: 0.7738\n",
      "Epoch 46\n",
      "Train Loss: 0.0205 | Acc: 0.9934 | F1: 0.9801\n",
      "Val   Loss: 0.3965 | Acc: 0.9364 | F1: 0.8147\n",
      "Epoch 47\n",
      "Train Loss: 0.0075 | Acc: 0.9975 | F1: 0.9924\n",
      "Val   Loss: 0.4321 | Acc: 0.9380 | F1: 0.8125\n",
      "Epoch 48\n",
      "Train Loss: 0.0062 | Acc: 0.9977 | F1: 0.9930\n",
      "Val   Loss: 0.4908 | Acc: 0.9387 | F1: 0.8191\n",
      "Epoch 49\n",
      "Train Loss: 0.0055 | Acc: 0.9979 | F1: 0.9937\n",
      "Val   Loss: 0.5342 | Acc: 0.9380 | F1: 0.8172\n",
      "Epoch 50\n",
      "Train Loss: 0.0051 | Acc: 0.9981 | F1: 0.9943\n",
      "Val   Loss: 0.5740 | Acc: 0.9385 | F1: 0.8163\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc, train_f1 = train(model, train_iterator)\n",
    "    val_loss, val_acc, val_f1 = evaluate(model, val_iterator)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 723100,
     "sourceId": 1257215,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 762846,
     "sourceId": 1316384,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 29976,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
